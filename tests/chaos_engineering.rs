//! Chaos Engineering Test Suite
//!
//! This module contains comprehensive chaos tests that simulate real-world
//! production failures to validate database resilience.

use lightning_db::{Database, LightningDbConfig};
use rand::Rng;
use std::fs;
use std::sync::{
    atomic::{AtomicBool, AtomicU64, Ordering},
    Arc, Barrier,
};
use std::thread;
use std::time::{Duration, Instant};

/// Chaos test results tracking
#[derive(Default)]
struct ChaosMetrics {
    total_operations: AtomicU64,
    failed_operations: AtomicU64,
    data_corruptions: AtomicU64,
    successful_recoveries: AtomicU64,
    total_crashes: AtomicU64,
}

impl ChaosMetrics {
    fn print_summary(&self) {
        let total = self.total_operations.load(Ordering::Relaxed);
        let failed = self.failed_operations.load(Ordering::Relaxed);
        let corruptions = self.data_corruptions.load(Ordering::Relaxed);
        let recoveries = self.successful_recoveries.load(Ordering::Relaxed);
        let crashes = self.total_crashes.load(Ordering::Relaxed);

        println!("\nüìä Chaos Test Summary:");
        println!("   Total Operations: {}", total);
        println!(
            "   Failed Operations: {} ({:.2}%)",
            failed,
            (failed as f64 / total as f64) * 100.0
        );
        println!("   Data Corruptions: {}", corruptions);
        println!("   Successful Recoveries: {} / {}", recoveries, crashes);
        println!(
            "   Resilience Score: {:.1}%",
            ((total - failed - corruptions) as f64 / total as f64) * 100.0
        );
    }
}

/// Simulates random process crashes during operations
#[test]
fn test_random_crash_recovery() {
    println!("üî• Testing Random Crash Recovery...");

    let metrics = Arc::new(ChaosMetrics::default());
    let test_dir = tempfile::tempdir().unwrap();
    let db_path = test_dir.path().to_path_buf();

    for iteration in 0..10 {
        println!("\n  Iteration {}/10", iteration + 1);

        // Phase 1: Write data with random crashes
        let should_crash = Arc::new(AtomicBool::new(false));
        let crash_clone = should_crash.clone();
        let metrics_clone = metrics.clone();
        let path_clone = db_path.clone();

        let writer_handle = thread::spawn(move || {
            let config = LightningDbConfig {
                cache_size: 10 * 1024 * 1024,
                use_improved_wal: true,
                ..Default::default()
            };

            let db = Database::open(&path_clone, config).unwrap();
            let mut rng = rand::rng();

            for i in 0..1000 {
                metrics_clone
                    .total_operations
                    .fetch_add(1, Ordering::Relaxed);

                // Randomly decide to crash
                if rng.gen_bool(0.001) && i > 100 {
                    crash_clone.store(true, Ordering::Relaxed);
                    metrics_clone.total_crashes.fetch_add(1, Ordering::Relaxed);
                    println!("    üí• Simulating crash at operation {}", i);
                    std::process::abort(); // Simulate hard crash
                }

                let key = format!("crash_test_{}", i);
                let value = format!("value_{}_integrity_check", i);

                match db.put(key.as_bytes(), value.as_bytes()) {
                    Ok(_) => {}
                    Err(_) => {
                        metrics_clone
                            .failed_operations
                            .fetch_add(1, Ordering::Relaxed);
                    }
                }

                // Simulate some reads too
                if i > 0 && rng.gen_bool(0.3) {
                    let read_key = format!("crash_test_{}", rng.gen_range(0..i));
                    let _ = db.get(read_key.as_bytes());
                }
            }
        });

        // Wait for writer to complete or crash
        let _ = writer_handle.join();

        // Phase 2: Recover and validate data integrity
        println!("    üîß Attempting recovery...");
        let metrics_clone = metrics.clone();

        let _recovery_result = std::panic::catch_unwind(|| {
            let config = LightningDbConfig {
                cache_size: 10 * 1024 * 1024,
                use_improved_wal: true,
                ..Default::default()
            };

            match Database::open(&db_path, config) {
                Ok(db) => {
                    // Verify data integrity
                    let mut verified = 0;
                    let mut corrupted = 0;

                    for i in 0..1000 {
                        let key = format!("crash_test_{}", i);
                        match db.get(key.as_bytes()) {
                            Ok(Some(value)) => {
                                let expected = format!("value_{}_integrity_check", i);
                                if value == expected.as_bytes() {
                                    verified += 1;
                                } else {
                                    corrupted += 1;
                                    metrics_clone
                                        .data_corruptions
                                        .fetch_add(1, Ordering::Relaxed);
                                }
                            }
                            Ok(None) => {
                                // Key not found - might not have been written before crash
                            }
                            Err(_) => {
                                corrupted += 1;
                            }
                        }
                    }

                    println!(
                        "    ‚úÖ Recovery successful: {} verified, {} corrupted",
                        verified, corrupted
                    );
                    metrics_clone
                        .successful_recoveries
                        .fetch_add(1, Ordering::Relaxed);
                    true
                }
                Err(e) => {
                    println!("    ‚ùå Recovery failed: {}", e);
                    false
                }
            }
        });

        // Clean up for next iteration
        let _ = fs::remove_dir_all(&db_path);
        thread::sleep(Duration::from_millis(100));
    }

    metrics.print_summary();
    assert!(metrics.successful_recoveries.load(Ordering::Relaxed) > 0);
    assert_eq!(metrics.data_corruptions.load(Ordering::Relaxed), 0);
}

/// Tests database behavior under extreme memory pressure
#[test]
fn test_memory_pressure_resilience() {
    println!("üî• Testing Memory Pressure Resilience...");

    let test_dir = tempfile::tempdir().unwrap();
    let metrics = Arc::new(ChaosMetrics::default());

    // Create database with minimal cache
    let config = LightningDbConfig {
        cache_size: 1024 * 1024, // Only 1MB cache
        compression_enabled: true,
        ..Default::default()
    };

    let db = Arc::new(Database::open(test_dir.path(), config).unwrap());
    let running = Arc::new(AtomicBool::new(true));

    // Spawn multiple threads doing heavy operations
    let mut handles = vec![];

    for thread_id in 0..8 {
        let db_clone = db.clone();
        let running_clone = running.clone();
        let metrics_clone = metrics.clone();

        let handle = thread::spawn(move || {
            let mut rng = rand::rng();
            let mut local_ops = 0;

            while running_clone.load(Ordering::Relaxed) {
                // Generate large values to stress memory
                let value_size = rng.gen_range(1024..1024 * 1024); // 1KB to 1MB
                let key = format!("memory_stress_{}_{}", thread_id, local_ops);
                let value: Vec<u8> = (0..value_size).map(|_| rng.gen()).collect();

                metrics_clone
                    .total_operations
                    .fetch_add(1, Ordering::Relaxed);

                // Try to write
                match db_clone.put(key.as_bytes(), &value) {
                    Ok(_) => {
                        // Immediately try to read it back
                        match db_clone.get(key.as_bytes()) {
                            Ok(Some(read_value)) => {
                                if read_value != value {
                                    metrics_clone
                                        .data_corruptions
                                        .fetch_add(1, Ordering::Relaxed);
                                }
                            }
                            Ok(None) => {
                                metrics_clone
                                    .failed_operations
                                    .fetch_add(1, Ordering::Relaxed);
                            }
                            Err(_) => {
                                metrics_clone
                                    .failed_operations
                                    .fetch_add(1, Ordering::Relaxed);
                            }
                        }
                    }
                    Err(_) => {
                        metrics_clone
                            .failed_operations
                            .fetch_add(1, Ordering::Relaxed);
                    }
                }

                local_ops += 1;

                // Occasionally force cache eviction
                if local_ops % 100 == 0 {
                    db_clone.sync().ok();
                }
            }
        });

        handles.push(handle);
    }

    // Run for 5 seconds
    thread::sleep(Duration::from_secs(5));
    running.store(false, Ordering::Relaxed);

    // Wait for all threads
    for handle in handles {
        handle.join().unwrap();
    }

    metrics.print_summary();
    assert_eq!(metrics.data_corruptions.load(Ordering::Relaxed), 0);
}

/// Tests concurrent transaction conflicts and resolution
#[test]
fn test_transaction_chaos() {
    println!("üî• Testing Transaction Chaos...");

    let test_dir = tempfile::tempdir().unwrap();
    let metrics = Arc::new(ChaosMetrics::default());

    let config = LightningDbConfig {
        cache_size: 50 * 1024 * 1024,
        use_improved_wal: true,
        ..Default::default()
    };

    let db = Arc::new(Database::open(test_dir.path(), config).unwrap());

    // Initialize accounts
    for i in 0..100 {
        db.put(format!("account_{}", i).as_bytes(), &1000u64.to_le_bytes())
            .unwrap();
    }

    let barrier = Arc::new(Barrier::new(16));
    let mut handles = vec![];

    // Spawn threads doing random transfers
    for thread_id in 0..16 {
        let db_clone = db.clone();
        let barrier_clone = barrier.clone();
        let metrics_clone = metrics.clone();

        let handle = thread::spawn(move || {
            let mut rng = rand::rng();
            barrier_clone.wait();

            for _ in 0..1000 {
                metrics_clone
                    .total_operations
                    .fetch_add(1, Ordering::Relaxed);

                // Random accounts
                let from = rng.gen_range(0..100);
                let to = rng.gen_range(0..100);
                if from == to {
                    continue;
                }

                let amount = rng.gen_range(1..100);

                // Start transaction
                let tx_id = match db_clone.begin_transaction() {
                    Ok(id) => id,
                    Err(_) => {
                        metrics_clone
                            .failed_operations
                            .fetch_add(1, Ordering::Relaxed);
                        continue;
                    }
                };

                let from_key = format!("account_{}", from);
                let to_key = format!("account_{}", to);

                // Read balances
                let from_balance = match db_clone.get_tx(tx_id, from_key.as_bytes()) {
                    Ok(Some(data)) if data.len() == 8 => {
                        u64::from_le_bytes(data[..8].try_into().unwrap())
                    }
                    _ => {
                        let _ = db_clone.abort_transaction(tx_id);
                        metrics_clone
                            .failed_operations
                            .fetch_add(1, Ordering::Relaxed);
                        continue;
                    }
                };

                let to_balance = match db_clone.get_tx(tx_id, to_key.as_bytes()) {
                    Ok(Some(data)) if data.len() == 8 => {
                        u64::from_le_bytes(data[..8].try_into().unwrap())
                    }
                    _ => {
                        let _ = db_clone.abort_transaction(tx_id);
                        metrics_clone
                            .failed_operations
                            .fetch_add(1, Ordering::Relaxed);
                        continue;
                    }
                };

                if from_balance < amount {
                    let _ = db_clone.abort_transaction(tx_id);
                    continue; // Insufficient funds
                }

                // Perform transfer
                if db_clone
                    .put_tx(
                        tx_id,
                        from_key.as_bytes(),
                        &(from_balance - amount).to_le_bytes(),
                    )
                    .is_err()
                    || db_clone
                        .put_tx(
                            tx_id,
                            to_key.as_bytes(),
                            &(to_balance + amount).to_le_bytes(),
                        )
                        .is_err()
                {
                    let _ = db_clone.abort_transaction(tx_id);
                    metrics_clone
                        .failed_operations
                        .fetch_add(1, Ordering::Relaxed);
                    continue;
                }

                // Random delay to increase conflict probability
                if rng.gen_bool(0.1) {
                    thread::sleep(Duration::from_micros(rng.gen_range(1..100)));
                }

                // Try to commit
                match db_clone.commit_transaction(tx_id) {
                    Ok(_) => {
                        // Verify invariant
                        let mut total = 0u64;
                        for i in 0..100 {
                            if let Ok(Some(data)) =
                                db_clone.get(format!("account_{}", i).as_bytes())
                            {
                                if data.len() == 8 {
                                    total += u64::from_le_bytes(data[..8].try_into().unwrap());
                                }
                            }
                        }
                        if total != 100_000 {
                            metrics_clone
                                .data_corruptions
                                .fetch_add(1, Ordering::Relaxed);
                            println!("    ‚ö†Ô∏è  Invariant violation: total = {}", total);
                        }
                    }
                    Err(_) => {
                        // Transaction conflict - this is expected
                    }
                }
            }
        });

        handles.push(handle);
    }

    // Wait for all threads
    for handle in handles {
        handle.join().unwrap();
    }

    // Final verification
    let mut total = 0u64;
    for i in 0..100 {
        if let Ok(Some(data)) = db.get(format!("account_{}", i).as_bytes()) {
            if data.len() == 8 {
                total += u64::from_le_bytes(data[..8].try_into().unwrap());
            }
        }
    }

    println!("\n  üí∞ Final total: {} (expected: 100000)", total);

    metrics.print_summary();
    assert_eq!(total, 100_000, "Money was created or destroyed!");
    assert_eq!(metrics.data_corruptions.load(Ordering::Relaxed), 0);
}

/// Tests disk corruption resilience
#[test]
fn test_disk_corruption_detection() {
    println!("üî• Testing Disk Corruption Detection...");

    let test_dir = tempfile::tempdir().unwrap();
    let db_path = test_dir.path().to_path_buf();
    let metrics = Arc::new(ChaosMetrics::default());

    // Phase 1: Write test data
    {
        let config = LightningDbConfig {
            cache_size: 10 * 1024 * 1024,
            use_improved_wal: true,
            ..Default::default()
        };

        let db = Database::open(&db_path, config).unwrap();

        for i in 0..1000 {
            let key = format!("corruption_test_{}", i);
            let value = format!("value_{}_with_checksum", i);
            db.put(key.as_bytes(), value.as_bytes()).unwrap();
            metrics.total_operations.fetch_add(1, Ordering::Relaxed);
        }

        db.sync().unwrap();
    }

    // Phase 2: Corrupt some data files
    println!("    üí£ Corrupting data files...");
    let mut corrupted_files = 0;

    if let Ok(entries) = fs::read_dir(&db_path) {
        for entry in entries.flatten() {
            let path = entry.path();
            if path.extension().and_then(|s| s.to_str()) == Some("db") {
                // Randomly corrupt some bytes
                if let Ok(mut data) = fs::read(&path) {
                    let mut rng = rand::rng();
                    if rng.gen_bool(0.3) && data.len() > 100 {
                        // Corrupt random positions
                        for _ in 0..5 {
                            let pos = rng.gen_range(0..data.len());
                            data[pos] = rng.gen();
                        }
                        fs::write(&path, data).ok();
                        corrupted_files += 1;
                    }
                }
            }
        }
    }

    println!("    üìù Corrupted {} files", corrupted_files);

    // Phase 3: Try to open and recover
    println!("    üîß Attempting to open corrupted database...");

    let config = LightningDbConfig {
        cache_size: 10 * 1024 * 1024,
        use_improved_wal: true,
        ..Default::default()
    };

    match Database::open(&db_path, config) {
        Ok(db) => {
            println!("    ‚úÖ Database opened despite corruption");

            // Verify what data we can still read
            let mut successful_reads = 0;
            let mut failed_reads = 0;

            for i in 0..1000 {
                let key = format!("corruption_test_{}", i);
                match db.get(key.as_bytes()) {
                    Ok(Some(value)) => {
                        let expected = format!("value_{}_with_checksum", i);
                        if value == expected.as_bytes() {
                            successful_reads += 1;
                        } else {
                            metrics.data_corruptions.fetch_add(1, Ordering::Relaxed);
                            failed_reads += 1;
                        }
                    }
                    Ok(None) => failed_reads += 1,
                    Err(_) => {
                        metrics.failed_operations.fetch_add(1, Ordering::Relaxed);
                        failed_reads += 1;
                    }
                }
            }

            println!(
                "    üìä Reads: {} successful, {} failed",
                successful_reads, failed_reads
            );

            if corrupted_files > 0 && failed_reads == 0 {
                println!("    üéâ Perfect recovery from corruption!");
                metrics
                    .successful_recoveries
                    .fetch_add(1, Ordering::Relaxed);
            }
        }
        Err(e) => {
            println!("    ‚ùå Failed to open: {}", e);
            if corrupted_files > 0 {
                println!("    ‚ÑπÔ∏è  This is expected with severe corruption");
            }
        }
    }

    metrics.print_summary();
}

/// Tests behavior during rapid open/close cycles
#[test]
fn test_rapid_lifecycle_chaos() {
    println!("üî• Testing Rapid Lifecycle Chaos...");

    let test_dir = tempfile::tempdir().unwrap();
    let db_path = test_dir.path().to_path_buf();
    let metrics = Arc::new(ChaosMetrics::default());
    let running = Arc::new(AtomicBool::new(true));

    let mut handles = vec![];

    // Writer thread
    let writer_path = db_path.clone();
    let writer_metrics = metrics.clone();
    let writer_running = running.clone();

    let writer_handle = thread::spawn(move || {
        let mut counter = 0;
        while writer_running.load(Ordering::Relaxed) {
            let config = LightningDbConfig {
                cache_size: 5 * 1024 * 1024,
                ..Default::default()
            };

            if let Ok(db) = Database::open(&writer_path, config) {
                // Write some data
                for _ in 0..100 {
                    let key = format!("lifecycle_{}", counter);
                    let value = format!("value_{}", counter);

                    writer_metrics
                        .total_operations
                        .fetch_add(1, Ordering::Relaxed);

                    if db.put(key.as_bytes(), value.as_bytes()).is_err() {
                        writer_metrics
                            .failed_operations
                            .fetch_add(1, Ordering::Relaxed);
                    }
                    counter += 1;
                }

                // Random sleep before closing
                thread::sleep(Duration::from_millis(rand::rng().gen_range(1..50)));
                drop(db);
            }

            // Random sleep before reopening
            thread::sleep(Duration::from_millis(rand::rng().gen_range(1..10)));
        }
    });

    handles.push(writer_handle);

    // Reader threads
    for _ in 0..3 {
        let reader_path = db_path.clone();
        let reader_metrics = metrics.clone();
        let reader_running = running.clone();

        let reader_handle = thread::spawn(move || {
            while reader_running.load(Ordering::Relaxed) {
                let config = LightningDbConfig {
                    cache_size: 5 * 1024 * 1024,
                    ..Default::default()
                };

                if let Ok(db) = Database::open(&reader_path, config) {
                    // Read random keys
                    for _ in 0..50 {
                        let key = format!("lifecycle_{}", rand::rng().gen_range(0..10000));

                        reader_metrics
                            .total_operations
                            .fetch_add(1, Ordering::Relaxed);

                        match db.get(key.as_bytes()) {
                            Ok(Some(value)) => {
                                // Verify format
                                if !value.starts_with(b"value_") {
                                    reader_metrics
                                        .data_corruptions
                                        .fetch_add(1, Ordering::Relaxed);
                                }
                            }
                            Ok(None) => {} // Key might not exist yet
                            Err(_) => {
                                reader_metrics
                                    .failed_operations
                                    .fetch_add(1, Ordering::Relaxed);
                            }
                        }
                    }

                    drop(db);
                }

                thread::sleep(Duration::from_millis(rand::rng().gen_range(1..20)));
            }
        });

        handles.push(reader_handle);
    }

    // Run for 10 seconds
    thread::sleep(Duration::from_secs(10));
    running.store(false, Ordering::Relaxed);

    // Wait for all threads
    for handle in handles {
        handle.join().unwrap();
    }

    // Final integrity check
    let config = LightningDbConfig {
        cache_size: 10 * 1024 * 1024,
        ..Default::default()
    };

    if let Ok(_db) = Database::open(&db_path, config) {
        println!("    ‚úÖ Database intact after chaos");
        metrics
            .successful_recoveries
            .fetch_add(1, Ordering::Relaxed);
    }

    metrics.print_summary();
    assert_eq!(metrics.data_corruptions.load(Ordering::Relaxed), 0);
}

/// Master chaos test that runs all scenarios
#[test]
#[ignore] // Run with: cargo test --ignored test_chaos_suite
fn test_chaos_suite() {
    println!("\nüå™Ô∏è  LIGHTNING DB CHAOS ENGINEERING SUITE\n");
    println!("This comprehensive test validates production resilience.\n");

    let start = Instant::now();

    // Run all chaos tests
    test_random_crash_recovery();
    println!("\n{}\n", "=".repeat(80));

    test_memory_pressure_resilience();
    println!("\n{}\n", "=".repeat(80));

    test_transaction_chaos();
    println!("\n{}\n", "=".repeat(80));

    test_disk_corruption_detection();
    println!("\n{}\n", "=".repeat(80));

    test_rapid_lifecycle_chaos();

    let duration = start.elapsed();
    println!(
        "\nüèÅ Chaos suite completed in {:.2}s",
        duration.as_secs_f64()
    );
    println!("‚úÖ Lightning DB demonstrated production-grade resilience!");
}
