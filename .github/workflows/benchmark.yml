name: Performance Benchmarks

on:
  push:
    branches: [ master, main ]
  pull_request:
    branches: [ master, main ]
  schedule:
    # Run benchmarks daily at 2 AM UTC
    - cron: '0 2 * * *'
  workflow_dispatch:
    inputs:
      benchmark_type:
        description: 'Type of benchmark to run'
        required: true
        default: 'all'
        type: choice
        options:
        - all
        - basic
        - stress
        - comparison

env:
  CARGO_TERM_COLOR: always
  RUST_BACKTRACE: 1

jobs:
  benchmark-matrix:
    name: Performance Benchmarks
    runs-on: ${{ matrix.os }}
    strategy:
      matrix:
        os: [ubuntu-latest, macos-latest, windows-latest]
        rust-version: [stable, nightly]
    
    steps:
    - uses: actions/checkout@v4
    
    - name: Install Rust
      uses: dtolnay/rust-toolchain@master
      with:
        toolchain: ${{ matrix.rust-version }}
    
    - name: Install system dependencies (Ubuntu)
      if: matrix.os == 'ubuntu-latest'
      run: |
        sudo apt-get update
        sudo apt-get install -y protobuf-compiler valgrind
    
    - name: Install system dependencies (macOS)
      if: matrix.os == 'macos-latest'
      run: |
        brew install protobuf
    
    - name: Install system dependencies (Windows)
      if: matrix.os == 'windows-latest'
      run: |
        choco install protoc
    
    - uses: Swatinem/rust-cache@v2
      with:
        key: benchmark-${{ matrix.os }}-${{ matrix.rust-version }}
    
    - name: Install criterion
      run: cargo install cargo-criterion
    
    - name: Run basic benchmarks
      if: github.event.inputs.benchmark_type == 'basic' || github.event.inputs.benchmark_type == 'all' || github.event.inputs.benchmark_type == ''
      run: |
        cargo bench --bench basic_benchmarks -- --output-format json > basic_results_${{ matrix.os }}_${{ matrix.rust-version }}.json
    
    - name: Run stress benchmarks
      if: github.event.inputs.benchmark_type == 'stress' || github.event.inputs.benchmark_type == 'all' || github.event.inputs.benchmark_type == ''
      run: |
        cargo bench --bench stress_benchmarks -- --output-format json > stress_results_${{ matrix.os }}_${{ matrix.rust-version }}.json
    
    - name: Run comparison benchmarks
      if: github.event.inputs.benchmark_type == 'comparison' || github.event.inputs.benchmark_type == 'all' || github.event.inputs.benchmark_type == ''
      run: |
        cargo bench --bench comparison_benchmarks -- --output-format json > comparison_results_${{ matrix.os }}_${{ matrix.rust-version }}.json
    
    - name: Memory usage analysis (Linux only)
      if: matrix.os == 'ubuntu-latest'
      run: |
        # Run memory benchmarks with Valgrind
        cargo build --release --example memory_analysis
        echo "=== Memory Usage Analysis ===" > memory_report_${{ matrix.rust-version }}.txt
        valgrind --tool=massif --massif-out-file=massif.out target/release/examples/memory_analysis
        ms_print massif.out >> memory_report_${{ matrix.rust-version }}.txt
    
    - name: Performance regression check
      run: |
        cargo run --release --example performance_check >> performance_summary_${{ matrix.os }}_${{ matrix.rust-version }}.txt
    
    - name: Upload benchmark results
      uses: actions/upload-artifact@v4
      with:
        name: benchmark-results-${{ matrix.os }}-${{ matrix.rust-version }}
        path: |
          *_results_*.json
          *_report_*.txt
          performance_summary_*.txt
        retention-days: 30
        
  benchmark-analysis:
    name: Analyze Performance
    runs-on: ubuntu-latest
    needs: benchmark-matrix
    steps:
    - uses: actions/checkout@v4
    
    - uses: actions/setup-python@v4
      with:
        python-version: '3.11'
    
    - name: Install analysis dependencies
      run: |
        pip install pandas matplotlib seaborn numpy scipy
    
    - name: Download all benchmark results
      uses: actions/download-artifact@v4
      with:
        path: benchmark-results/
    
    - name: Generate performance report
      run: |
        python scripts/analyze_benchmarks.py benchmark-results/ > performance_report.md
    
    - name: Performance regression detection
      run: |
        python scripts/detect_regressions.py benchmark-results/ > regression_report.md
    
    - name: Create performance charts
      run: |
        python scripts/create_charts.py benchmark-results/ charts/
    
    - name: Upload analysis results
      uses: actions/upload-artifact@v4
      with:
        name: performance-analysis
        path: |
          performance_report.md
          regression_report.md
          charts/
    
    - name: Comment PR with results
      if: github.event_name == 'pull_request'
      uses: actions/github-script@v7
      with:
        script: |
          const fs = require('fs');
          
          try {
            const report = fs.readFileSync('performance_report.md', 'utf8');
            const regression = fs.readFileSync('regression_report.md', 'utf8');
            
            const body = `## 🚀 Performance Benchmark Results
            
            ${report}
            
            ## 🔍 Regression Analysis
            
            ${regression}
            
            <details>
            <summary>📊 View Performance Charts</summary>
            
            Performance charts have been generated and are available in the artifacts.
            
            </details>`;
            
            github.rest.issues.createComment({
              issue_number: context.issue.number,
              owner: context.repo.owner,
              repo: context.repo.repo,
              body: body
            });
          } catch (error) {
            console.log('Could not read performance reports:', error);
          }
          
  benchmark-store:
    name: Store Historical Data
    runs-on: ubuntu-latest
    needs: [benchmark-matrix, benchmark-analysis]
    if: github.ref == 'refs/heads/master' || github.ref == 'refs/heads/main'
    steps:
    - uses: actions/checkout@v4
      with:
        fetch-depth: 0
    
    - name: Download benchmark results
      uses: actions/download-artifact@v4
      with:
        path: benchmark-results/
    
    - name: Store in gh-pages branch
      run: |
        # Create performance history
        git config user.name "GitHub Actions"
        git config user.email "actions@github.com"
        
        # Create or switch to gh-pages branch
        git checkout gh-pages || git checkout --orphan gh-pages
        
        # Create directory structure
        mkdir -p benchmarks/$(date +%Y/%m)
        
        # Copy results
        cp -r benchmark-results/* benchmarks/$(date +%Y/%m)/
        
        # Update index
        python scripts/update_benchmark_index.py
        
        # Commit and push
        git add .
        git commit -m "Add benchmark results for $(date +%Y-%m-%d)"
        git push origin gh-pages
        
  alert-on-regression:
    name: Alert on Performance Regression
    runs-on: ubuntu-latest
    needs: benchmark-analysis
    if: github.ref == 'refs/heads/master' || github.ref == 'refs/heads/main'
    steps:
    - name: Download analysis results
      uses: actions/download-artifact@v4
      with:
        name: performance-analysis
        path: ./
    
    - name: Check for regressions
      id: regression-check
      run: |
        if grep -q "REGRESSION DETECTED" regression_report.md; then
          echo "regression=true" >> $GITHUB_OUTPUT
          echo "PERFORMANCE REGRESSION DETECTED!"
        else
          echo "regression=false" >> $GITHUB_OUTPUT
          echo "No significant performance regressions detected."
        fi
    
    - name: Create issue on regression
      if: steps.regression-check.outputs.regression == 'true'
      uses: actions/github-script@v7
      with:
        script: |
          const fs = require('fs');
          const regression = fs.readFileSync('regression_report.md', 'utf8');
          
          github.rest.issues.create({
            owner: context.repo.owner,
            repo: context.repo.repo,
            title: '🚨 Performance Regression Detected',
            body: `## Performance Regression Alert
            
            A significant performance regression has been detected in the latest benchmarks.
            
            ${regression}
            
            ### Action Required
            - Review recent changes that might impact performance
            - Run local benchmarks to confirm the regression
            - Investigate and fix performance issues
            - Re-run benchmarks to verify fixes
            
            ### Benchmark Results
            See the latest benchmark artifacts for detailed performance data.
            `,
            labels: ['performance', 'regression', 'high-priority']
          });
          
  comparison-benchmarks:
    name: Database Comparison
    runs-on: ubuntu-latest
    steps:
    - uses: actions/checkout@v4
    
    - name: Install Rust
      uses: dtolnay/rust-toolchain@stable
    
    - uses: Swatinem/rust-cache@v2
    
    - name: Install comparison databases
      run: |
        # Install SQLite
        sudo apt-get update
        sudo apt-get install -y sqlite3 libsqlite3-dev
        
        # Add RocksDB, LevelDB dependencies
        sudo apt-get install -y librocksdb-dev libleveldb-dev
    
    - name: Run database comparison
      run: |
        cargo run --release --example database_comparison > comparison_results.txt
    
    - name: Generate comparison report
      run: |
        echo "## 📊 Database Performance Comparison" > comparison_report.md
        echo "" >> comparison_report.md
        echo "\`\`\`" >> comparison_report.md
        cat comparison_results.txt >> comparison_report.md
        echo "\`\`\`" >> comparison_report.md
    
    - name: Upload comparison results
      uses: actions/upload-artifact@v4
      with:
        name: database-comparison
        path: |
          comparison_results.txt
          comparison_report.md